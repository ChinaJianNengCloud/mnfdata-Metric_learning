# -*- coding: utf-8 -*-
"""Inference

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/Inference.ipynb

# PyTorch Metric Learning
See the documentation [here](https://kevinmusgrave.github.io/pytorch-metric-learning/)

## Install the packages
"""


"""## Import the packages"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import umap
import numpy as np
import torch
import torchvision
import resnet
from torchvision import datasets, transforms
import torch.nn as nn
from scipy.special import softmax
from torch.utils.data import DataLoader

from pytorch_metric_learning.distances import CosineSimilarity
from pytorch_metric_learning.utils import common_functions as c_f
from pytorch_metric_learning.utils.inference import InferenceModel, MatchFinder
import pandas as pd
from sklearn.metrics import roc_auc_score,accuracy_score
from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator, precision_at_k
import pytorch_metric_learning.utils.logging_presets as logging_presets
from pytorch_metric_learning import losses, miners, samplers, testers, trainers
"""## Create helper functions"""
class Yinda_calculator(AccuracyCalculator):
    def calculate_acc_yinda(self, knn_labels, query_labels, **kwargs):
        # 将 query_labels 转换为一维数组
        data=kwargs['classifier_and_labels']
        data_logit = data[0].cpu().numpy()
        labels = data[1].cpu().numpy()
        argmax_indices = np.argmax(data_logit, axis=1)
        # 计算准确率
        return accuracy_score(labels,argmax_indices )

    def requires_knn(self):
        return super().requires_knn() + ["acc_yinda", "auc_yinda"]

    def calculate_auc_yinda(self, knn_labels, query_labels, **kwargs):
        data = kwargs['classifier_and_labels']
        labels = data[1].cpu().numpy()  # 真实标签
        data_logit = data[0].cpu().numpy()  # 预测分数

        # 然后使用归一化后的值计算置信度分数
        probabilities = softmax(data_logit, axis=1)

        # 获取正类的概率
        positive_class_probabilities = probabilities[:, 1]

        # 使用 sklearn 计算 AUC
        auc = roc_auc_score(labels, positive_class_probabilities)

        return auc




def print_decision(is_match):
    if is_match:
        print("Same class")
    else:
        print("Different class")

class MLP(nn.Module):
    # layer_sizes[0] is the dimension of the input
    # layer_sizes[-1] is the dimension of the output
    def __init__(self, layer_sizes, final_relu=False):
        super().__init__()
        layer_list = []
        layer_sizes = [int(x) for x in layer_sizes]
        num_layers = len(layer_sizes) - 1
        final_relu_layer = num_layers if final_relu else num_layers - 1
        for i in range(len(layer_sizes) - 1):
            input_size = layer_sizes[i]
            curr_size = layer_sizes[i + 1]
            if i < final_relu_layer:
                layer_list.append(nn.ReLU(inplace=False))
            layer_list.append(nn.Linear(input_size, curr_size))
        self.net = nn.Sequential(*layer_list)
        self.last_linear = self.net[-1]

    def forward(self, x):
        return self.net(x)

class Conv1DModel(nn.Module):
    def __init__(self):
        super(Conv1DModel, self).__init__()
        # 定义一维卷积层
        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)
        self.conv3 = nn.Conv1d(in_channels=32, out_channels=1, kernel_size=3, padding=1)
        # 定义激活函数
        self.relu = nn.ReLU()

    def forward(self, x):
        # 将输入形状调整为 (batch_size, channels, length)
        x = x.unsqueeze(1)
        # 通过卷积层和激活函数
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.conv3(x)
        # 去掉通道维度
        x = x.squeeze(1)
        return x


class mnfDataset(torch.utils.data.Dataset):
    def __init__(self, file_path, sheet_name, transform=None, device=None):
        # 读取Excel文件
        self.tmp = pd.read_excel(file_path, sheet_name=sheet_name)

        # 将数据转换为NumPy数组，并限制小数位数
        self.data = np.round(self.tmp.iloc[:, 1:33].to_numpy(dtype=np.float32), decimals=16)
        self.targets = self.tmp.iloc[:, 0].to_numpy(dtype=np.float32)
        self.transform = transform
        self.device = device if device else torch.device('cpu')

        # 统一数值表示
        self.data = self._convert_to_decimal(self.data)

        # 标准化数据形状
        self.data = self._standardize_data(self.data)

    def _convert_to_decimal(self, data):
        # 将所有数值转换为统一的小数表示
        return np.array([[float(f"{value:.16f}") for value in row] for row in data], dtype=np.float32)

    def _standardize_data(self, data):
        max_length = max(len(row) for row in data)
        standardized_data = np.zeros((len(data), max_length), dtype=np.float32)
        for i, row in enumerate(data):
            standardized_data[i, :len(row)] = row
        return standardized_data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        data = self.data[index]
        target = self.targets[index]

        if self.transform is not None:
            data = self.transform(data)
        return data, target


if __name__ == '__main__':
    a=1
    if (a==1):
        name="训练集"
    elif(a==2):
        name="留出集"
    elif(a==3):
        name="西溪验证集"
    elif(a==4):
        name="浙一验证集"
    log_dir = r"E:\yinda\mnf\result\tmp\训练集"
    tensorboard_dir = r"E:\yinda\mnf\result\tmp\训练集"
    model_save_dir = r"E:\yinda\mnf\result\tmp\训练集"
    batch_size = 32
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    trunk_output_size = 32  # 根据你的输入维度来设定
    trunk = MLP([trunk_output_size, 64, 64, 64, 32]).to(device)
    # trunk = Conv1DModel().to(device)
    embedder = MLP([trunk_output_size, 64]).to(device)
    classifier = MLP([64, 2]).to(device)

    test_dataset = mnfDataset(file_path='E:\yinda\mnf\归一化后的数据.xlsx', sheet_name='训练集', transform=None, device=device)
    # dataloader = DataLoader(test_dataset, batch_size=1024, shuffle=True, num_workers=4)

    labels_to_indices = c_f.get_labels_to_indices(test_dataset.targets)

    checkpoint_path_trunk = r"E:\yinda\mnf\result\tmp\model\trunk_best98.pth"
    checkpoint_path_embedder = r"E:\yinda\mnf\result\tmp\model\embedder_best98.pth"
    checkpoint_path_classifier = r"E:\yinda\mnf\result\tmp\model\classifier_best98.pth"

    checkpoint_trunk = torch.load(checkpoint_path_trunk)
    checkpoint_embedder = torch.load(checkpoint_path_embedder)
    checkpoint_classifier = torch.load(checkpoint_path_classifier)

    trunk.load_state_dict(checkpoint_trunk)
    trunk = torch.nn.DataParallel(trunk).to(device)

    embedder.load_state_dict(checkpoint_embedder)
    embedder = torch.nn.DataParallel(embedder).to(device)

    classifier.load_state_dict(checkpoint_classifier)
    classifier = torch.nn.DataParallel(classifier).to(device)

    # trunk.to(device)
    # embedder.to(device)
    # classifier.to(device)

    print("done model loading")

    """## Create the InferenceModel wrapper"""

    match_finder = MatchFinder(distance=CosineSimilarity(), threshold=0.7)

    inference_model = InferenceModel(trunk,embedder,classifier, match_finder=match_finder)

    """## Get nearest neighbors of a query"""

    # create faiss index
    inference_model.train_knn(test_dataset)
    record_keeper, _, _ = logging_presets.get_record_keeper(
        log_dir, tensorboard_dir
    )
    hooks = logging_presets.get_hook_container(record_keeper)
    metric_yinda = Yinda_calculator(
        include=(
            "precision_at_1",
            "r_precision",
            "mean_average_precision_at_r",
            "acc_yinda",
            "auc_yinda"
        )
    )

    # Create the tester
    tester = testers.GlobalEmbeddingSpaceTester(
        batch_size=batch_size,
        end_of_testing_hook=hooks.end_of_testing_hook,
        visualizer=umap.UMAP(),
        visualizer_hook=None,
        dataloader_num_workers=1,
        accuracy_calculator=metric_yinda,
    )
    dataset_dict={"val":test_dataset}
    epoch=0
    tester.test(
        dataset_dict,
        epoch,
        trunk,
        embedder,
        classifier
    )